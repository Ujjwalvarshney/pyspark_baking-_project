{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "sc = SparkContext(appName=\"PythonInclassStreaming\")\n",
    "ssc = StreamingContext(sc, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Receiver\n",
    "kvs = KafkaUtils.createStream(ssc, \"ip-20-0-21-161.ec2.internal:2181\", \"spark-streaming-consumer\", {\"ujjwal_data\": 1})\n",
    "#kvs = KafkaUtils.createStream(ssc, \"localhost:2181\", \"spark-streaming-consumer\", {\"end_to_end\": 1})\n",
    "\n",
    "# Dstream\n",
    "lines = kvs.map(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process(t, rdd):\n",
    "    if rdd.isEmpty():\n",
    "        print(\"Empty Rdd\")\n",
    "        return\n",
    "    \n",
    "    spark=SparkSession.builder.getOrCreate()\n",
    "    rawRdd = rdd.map(lambda x: x.split(',')).map(lambda d: Row(NAME_CONTRACT_TYPE=d[0], \n",
    "                                                               CODE_GENDER=d[1], \n",
    "                                                               FLAG_OWN_CAR=d[2], \n",
    "                                                               FLAG_OWN_REALTY=d[3], \n",
    "                                                               CNT_CHILDREN=int(d[4]), \n",
    "                                                               AMT_INCOME_TOTAL=float(d[5]), \n",
    "                                                               AMT_CREDIT=float(d[6]), \n",
    "                                                               AMT_ANNUITY=float(d[7]), \n",
    "                                                               NAME_INCOME_TYPE=d[8], \n",
    "                                                               NAME_EDUCATION_TYPE=d[9],\n",
    "                                                               NAME_FAMILY_STATUS=d[10], \n",
    "                                                               NAME_HOUSING_TYPE=d[11], \n",
    "                                                               DAYS_BIRTH=int(d[12]), \n",
    "                                                               DAYS_EMPLOYED=int(d[13]), \n",
    "                                                               FLAG_MOBIL=int(d[14]), \n",
    "                                                               FLAG_EMP_PHONE=int(d[15]), \n",
    "                                                               FLAG_WORK_PHONE=int(d[16]), \n",
    "                                                               FLAG_CONT_MOBILE=int(d[17]), \n",
    "                                                               FLAG_PHONE=int(d[18]), \n",
    "                                                               CNT_FAM_MEMBERS=float(d[19]), \n",
    "                                                               REGION_RATING_CLIENT=int(d[20]), \n",
    "                                                               REGION_RATING_CLIENT_W_CITY=int(d[21]), \n",
    "                                                               REG_REGION_NOT_LIVE_REGION=int(d[22]), \n",
    "                                                               REG_REGION_NOT_WORK_REGION=int(d[23]), \n",
    "                                                               ORGANIZATION_TYPE=d[24], \n",
    "                                                               FLAG_DOCUMENT_2=int(d[25]), \n",
    "                                                               FLAG_DOCUMENT_3=int(d[26]), \n",
    "                                                               FLAG_DOCUMENT_4=int(d[27]), \n",
    "                                                               FLAG_DOCUMENT_5=int(d[28]), \n",
    "                                                               FLAG_DOCUMENT_6=int(d[29]), \n",
    "                                                               FLAG_DOCUMENT_7=int(d[30]), \n",
    "                                                               FLAG_DOCUMENT_8=int(d[31]), \n",
    "                                                               FLAG_DOCUMENT_9=int(d[32]), \n",
    "                                                               FLAG_DOCUMENT_10=int(d[33]), \n",
    "                                                               FLAG_DOCUMENT_11=int(d[34]), \n",
    "                                                               FLAG_DOCUMENT_12=int(d[35])))\n",
    "    raw = spark.createDataFrame(rawRdd)\n",
    "    print(raw.show(5))\n",
    "    if not rdd.isEmpty():\n",
    "        df = rdd.map(lambda r:Row(message=r))\n",
    "        df = df.withColumn('CREDIT_INCOME_PERCENT',F.col('AMT_CREDIT')/F.col('AMT_INCOME_TOTAL'))\n",
    "        df = df.withColumn(\"AGE\", F.col(\"DAYS_BIRTH\")/-365)\n",
    "        df = df.withColumn('DAYS_EMPLOYED_ANOM',F.col(\"DAYS_EMPLOYED\") == 365243)\n",
    "        df = df.withColumn('DAYS_EMPLOYED', F.when(F.col('DAYS_EMPLOYED') == 365243, 0).otherwise(F.col('DAYS_EMPLOYED')))\n",
    "        df = df.withColumn('CREDIT_INCOME_PERCENT',F.col('AMT_CREDIT')/F.col('AMT_INCOME_TOTAL'))\n",
    "        df = df.withColumn('ANNUITY_INCOME_PERCENT',F.col('AMT_ANNUITY')/F.col('AMT_INCOME_TOTAL'))\n",
    "        df = df.withColumn('CREDIT_TERM',F.col('AMT_ANNUITY')/F.col('AMT_CREDIT'))\n",
    "        df = df.withColumn('DAYS_EMPLOYED_PERCENT',F.col('DAYS_EMPLOYED')/F.col('DAYS_BIRTH'))\n",
    "        pipeline = PipelineModel.load('hdfs:///user/edureka_1033463/pymodellr_ujjwal')\n",
    "        predictions = pipeline.transform(df)\n",
    "        print(predictions.select('prediction').show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines.pprint()\n",
    "lines.foreachRDD(process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2020-06-13 05:57:55\n",
      "-------------------------------------------\n",
      "\n",
      "Empty Rdd\n",
      "-------------------------------------------\n",
      "Time: 2020-06-13 05:58:00\n",
      "-------------------------------------------\n",
      "\n",
      "Empty Rdd\n",
      "-------------------------------------------\n",
      "Time: 2020-06-13 05:58:05\n",
      "-------------------------------------------\n",
      "Cash,M,N,Y,0,202500.0,406597.5,24700.5,Working,Secondary,Single,House,-9461,-637,1,1,0,1,1,1.0,2,2,0,0,Business Entity Type 3,0,1,0,0,0,0,0,0,0,0,0\n",
      "\n",
      "+-----------+----------+----------------+------------+---------------+-----------+----------+-------------+----------------+----------------+----------------+----------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+--------------+----------+------------+---------------+----------+---------------+------------------+-------------------+------------------+-----------------+----------------+--------------------+--------------------+---------------------------+--------------------------+--------------------------+\n",
      "|AMT_ANNUITY|AMT_CREDIT|AMT_INCOME_TOTAL|CNT_CHILDREN|CNT_FAM_MEMBERS|CODE_GENDER|DAYS_BIRTH|DAYS_EMPLOYED|FLAG_CONT_MOBILE|FLAG_DOCUMENT_10|FLAG_DOCUMENT_11|FLAG_DOCUMENT_12|FLAG_DOCUMENT_2|FLAG_DOCUMENT_3|FLAG_DOCUMENT_4|FLAG_DOCUMENT_5|FLAG_DOCUMENT_6|FLAG_DOCUMENT_7|FLAG_DOCUMENT_8|FLAG_DOCUMENT_9|FLAG_EMP_PHONE|FLAG_MOBIL|FLAG_OWN_CAR|FLAG_OWN_REALTY|FLAG_PHONE|FLAG_WORK_PHONE|NAME_CONTRACT_TYPE|NAME_EDUCATION_TYPE|NAME_FAMILY_STATUS|NAME_HOUSING_TYPE|NAME_INCOME_TYPE|   ORGANIZATION_TYPE|REGION_RATING_CLIENT|REGION_RATING_CLIENT_W_CITY|REG_REGION_NOT_LIVE_REGION|REG_REGION_NOT_WORK_REGION|\n",
      "+-----------+----------+----------------+------------+---------------+-----------+----------+-------------+----------------+----------------+----------------+----------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+--------------+----------+------------+---------------+----------+---------------+------------------+-------------------+------------------+-----------------+----------------+--------------------+--------------------+---------------------------+--------------------------+--------------------------+\n",
      "|    24700.5|  406597.5|        202500.0|           0|            1.0|          M|     -9461|         -637|               1|               0|               0|               0|              0|              1|              0|              0|              0|              0|              0|              0|             1|         1|           N|              Y|         1|              0|              Cash|          Secondary|            Single|            House|         Working|Business Entity T...|                   2|                          2|                         0|                         0|\n",
      "+-----------+----------+----------------+------------+---------------+-----------+----------+-------------+----------------+----------------+----------------+----------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+--------------+----------+------------+---------------+----------+---------------+------------------+-------------------+------------------+-----------------+----------------+--------------------+--------------------+---------------------------+--------------------------+--------------------------+\n",
      "\n",
      "None\n",
      "-------------------------------------------\n",
      "Time: 2020-06-13 05:58:10\n",
      "-------------------------------------------\n",
      "\n",
      "Empty Rdd\n",
      "-------------------------------------------\n",
      "Time: 2020-06-13 05:58:15\n",
      "-------------------------------------------\n",
      "\n",
      "Empty Rdd\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o48.awaitTermination.\n: org.apache.spark.SparkException: An exception was raised by Python:\nTraceback (most recent call last):\n  File \"/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/streaming/util.py\", line 65, in call\n    r = self.func(t, *rdds)\n  File \"<ipython-input-4-2b99f1855cb2>\", line 47, in process\n    df = df.withColumn('CREDIT_INCOME_PERCENT',F.col('AMT_CREDIT')/F.col('AMT_INCOME_TOTAL'))\nAttributeError: 'PipelinedRDD' object has no attribute 'withColumn'\n\n\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n\tat org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)\n\tat org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)\n\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:256)\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:255)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-d970cfd6a21f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/streaming/context.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \"\"\"\n\u001b[1;32m    205\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTerminationOrTimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o48.awaitTermination.\n: org.apache.spark.SparkException: An exception was raised by Python:\nTraceback (most recent call last):\n  File \"/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/streaming/util.py\", line 65, in call\n    r = self.func(t, *rdds)\n  File \"<ipython-input-4-2b99f1855cb2>\", line 47, in process\n    df = df.withColumn('CREDIT_INCOME_PERCENT',F.col('AMT_CREDIT')/F.col('AMT_INCOME_TOTAL'))\nAttributeError: 'PipelinedRDD' object has no attribute 'withColumn'\n\n\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n\tat org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)\n\tat org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)\n\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:256)\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:255)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2020-06-13 05:58:20\n",
      "-------------------------------------------\n",
      "\n",
      "Empty Rdd\n",
      "-------------------------------------------\n",
      "Time: 2020-06-13 05:58:25\n",
      "-------------------------------------------\n",
      "\n",
      "Empty Rdd\n",
      "-------------------------------------------\n",
      "Time: 2020-06-13 05:58:30\n",
      "-------------------------------------------\n",
      "\n",
      "Empty Rdd\n",
      "-------------------------------------------\n",
      "Time: 2020-06-13 05:58:35\n",
      "-------------------------------------------\n",
      "\n",
      "Empty Rdd\n",
      "-------------------------------------------\n",
      "Time: 2020-06-13 05:58:40\n",
      "-------------------------------------------\n",
      "\n",
      "Empty Rdd\n",
      "-------------------------------------------\n",
      "Time: 2020-06-13 05:58:45\n",
      "-------------------------------------------\n",
      "\n",
      "Empty Rdd\n",
      "-------------------------------------------\n",
      "Time: 2020-06-13 05:58:50\n",
      "-------------------------------------------\n",
      "\n",
      "Empty Rdd\n"
     ]
    }
   ],
   "source": [
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
